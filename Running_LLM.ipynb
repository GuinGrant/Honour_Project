{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip my_model_final.zip"
      ],
      "metadata": {
        "id": "wvkdaomNZu3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes transformers accelerate\n"
      ],
      "metadata": {
        "id": "DksPmm92P52v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "base_model_name = \"unsloth/mistral-7b-bnb-4bit\"  # Path to your base model\n",
        "adapter_model_name = \"my_model_final\"  # Path to your fine-tuned adapter model\n",
        "max_output_lines = 10                             # Maximum number of output lines to show\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load base model and adapter\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(model, adapter_model_name)\n",
        "model = model.bfloat16()\n",
        "model = model.to(device)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "tU_mbQMoaSbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop to continuously take user input and log input-output pairs\n",
        "while True:\n",
        "    input_text = input(\"Enter your prompt (or type 'exit' to quit): \")\n",
        "    if input_text.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "    outputs = model.generate(input_ids, max_new_tokens=1000)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    lines = generated_text.split('\\n')[:max_output_lines]\n",
        "    limited_text = '\\n'.join(lines)\n",
        "\n",
        "    print(\"Generated Output:\")\n",
        "    print(limited_text)\n",
        "    print(\"=\" * 50)  # Separator for readability\n",
        "\n",
        "    # Log input and output to a file\n",
        "    with open(\"conversation_log.txt\", \"a\", encoding=\"utf-8\") as log_file:\n",
        "        log_file.write(\"User Input:\\n\")\n",
        "        log_file.write(input_text + \"\\n\\n\")\n",
        "        log_file.write(\"Model Output:\\n\")\n",
        "        log_file.write(limited_text + \"\\n\")\n",
        "        log_file.write(\"=\" * 50 + \"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "stJeDc5ThJjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}